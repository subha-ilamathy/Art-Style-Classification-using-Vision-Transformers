# -*- coding: utf-8 -*-
"""Multimodal ViT and Clip based Art classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C5smpDVHDCLteF9RXWKjad1oZfTPSLN0

# Artwork Classifier

## Introduction

In this project, we will classify artworks using the [artbench](https://github.com/liaopeiyuan/artbench) dataset. The dataset includes 60,000 images of artworks from 10 different artistic styles, including paintings, murals, and sculptures from the 14th to the 21st century. Each style has 5,000 training images and 1,000 test images.

We will predict the classes using PyTorch and leveraging *convolutional* and *transformer-based* models, namely the Resnet and Vision Transformers.

## Table of Contents

I. [Define parameters](#define-param)<br>
II. [Get Data](#get_data)<br>
III. [Data preparation](#data-prep)<br>
IV. [ViT_B16: Fine Tuning](#vit_b16_FT)<br>
V. [Resnet-50](#resnet-50)<br>

## <a class="anchor" id="define-param">I. Define parameters</a>
"""

import torch
print(torch.__version__)
from torch import nn
from torch.utils.data import DataLoader

import torchvision
print(torchvision.__version__)
from torchvision import datasets,models,transforms
import torchinfo

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm.auto import tqdm
from timeit import default_timer as timer
from pathlib import Path
import random,os
import wget,tarfile

import warnings
warnings.simplefilter("ignore", UserWarning)

Path('kaggle/working/results').mkdir(parents=True, exist_ok=True)
Path('kaggle/working/models').mkdir(parents=True, exist_ok=True)
Path('kaggle/working/gradio_demo/').mkdir(parents=True, exist_ok=True)

#########################################
# Define Parameters
#########################################
FLAGS = {}
# FLAGS['datadir'] = "./"
FLAGS['datadir'] = "kaggle/working/"
FLAGS['batch_size'] = 32
# FLAGS['num_workers'] = os.cpu_count()
FLAGS['num_workers'] = 0
FLAGS['num_epochs'] = 5
FLAGS['show_results_every_X_batch'] = 100
FLAGS['IMAGE_SIZE'] = 224

FLAGS

PATH_results = Path(FLAGS['datadir'] + "results")
PATH_models = Path(FLAGS['datadir'] + "models")

device = "cuda:1" if torch.cuda.is_available() else "cpu"
device

"""## <a class="anchor" id="get_data">II. Get data</a>

There are three versions of the dataset, each with a different resolution: 32x32, 256x256, and the original.

We will download the 256x256 ImageFolder with train-test split from [here](https://github.com/liaopeiyuan/artbench).
"""

download = False # Set to True to start downloading

if download:
    # !wget https://artbench.eecs.berkeley.edu/files/artbench-10-imagefolder-split.tar

    import wget
    print("Download artbench-10-imagefolder-split.tar ...")
    wget.download(url="https://artbench.eecs.berkeley.edu/files/artbench-10-imagefolder-split.tar",
                  out = FLAGS['datadir'])

    # Uncompress the tar file
    import tarfile
    print("Uncompress the tar file...")
    data_path = Path(FLAGS['datadir'])
    file = tarfile.open(data_path / "artbench-10-imagefolder-split.tar")
    file.extractall(data_path)
    file.close()
    print("Tar file uncompressed!")

"""## <a class="anchor" id="data-prep">III. Data preparation</a>"""

data_path = Path(FLAGS['datadir'])
image_path = data_path / "artbench-10-imagefolder-split"

for dirpath, dirnames, filenames in os.walk(image_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

"""### Visualize an image"""

# 1. Get image paths
image_path_list = list(image_path.glob("train/surrealism/*.jpg"))

# 2. Get random image path
random_image_path = random.choice(image_path_list)

# 3. Get image class from path name
image_class = random_image_path.parent.stem

# 4. Open image
img = Image.open(random_image_path)

# 5. Print Image class, heigt and width
print(f"Image class: {image_class}")
print(f"Image height: {img.height}")
print(f"Image width: {img.width}")
img

"""### Transforming images

The `torchvision.transforms` module is used here to [transform and augment images](https://pytorch.org/vision/stable/transforms.html). A series of transforms is created using the `transforms.Compose` method.

The images are resized using `transforms.Resize()` and converted to tensor and scaled between 0 and 1 using `transforms.ToTensor()`.

To prevent overfitting, we will use [transforms.TrivialAugmentWide()](https://pytorch.org/vision/main/generated/torchvision.transforms.TrivialAugmentWide.html). This is an automatic augmentation method that transforms an image using a random transform from a given list with a random strength number between 0 and 31. We have found that a strength of 8 provides better accuracy and loss than higher magnitudes.
"""

data_transform = transforms.Compose([
transforms.Resize(size=(FLAGS['IMAGE_SIZE'], FLAGS['IMAGE_SIZE'])),
transforms.TrivialAugmentWide(num_magnitude_bins=8), # Data Augmentation
transforms.ToTensor()
])

# plot original vs. transformed image

with Image.open(random_image_path) as f:
  fig, ax = plt.subplots(1, 2,figsize=(10,7))
  ax[0].imshow(f)
  ax[0].set_title(f"Original \nSize: {f.size}")
  ax[0].axis("off")

  # Transform and plot image
  transformed_image = data_transform(f).permute(1, 2, 0)
  ax[1].imshow(transformed_image)
  ax[1].set_title(f"Transformed \nSize: {transformed_image.shape}")
  ax[1].axis("off")

"""### Loading Image Data

To load image data and convert it into a Dataset, we will use [`ImageFolder`](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html#torchvision.datasets.ImageFolder). The file path of the target image directory and our transform series are the main parameters of this method.
"""

# train_data = datasets.ImageFolder(root=image_path / "train", transform=data_transform)

# test_data = datasets.ImageFolder(root=image_path / "test", transform=data_transform)

# print(f"Train data:\n{train_data}\nTest data:\n{test_data}")

import pickle

# Load the dictionary from the file
with open('embeding.pkl', 'rb') as f:
    embed_dict = pickle.load(f)

with open('embeding_test.pkl', 'rb') as f:
    test_embed_dict = pickle.load(f)

import torch
from torchvision.datasets import ImageFolder
from torch.utils.data import Dataset
import pdb
class CustomImageTextDataset(Dataset):
    def __init__(self, img_folder:str, transform: transforms.Compose, embedding_dict: dict):
        """
        img_folder: Path to the image folder or an ImageFolder instance.
        transform: Transformations to be applied to the images.
        text_data: A list or array of text tensors corresponding to each image.
        """
        # Initialize the ImageFolder dataset
        self.img_folder = ImageFolder(root=img_folder, transform=transform)
        # pdb.set_trace()
        self.embedding_dict = embedding_dict

    def __len__(self):
        return len(self.img_folder)

    def __getitem__(self, idx):
        image, label = self.img_folder[idx]
        # '/'.join(self.img_folder.imgs[0][0].split('/')[-2:])
        text_tensor = torch.tensor(self.embedding_dict['/'.join(self.img_folder.imgs[0][0].split('/')[-2:])]).squeeze(1)
        # text_tensor = self.text_data[idx]
        return (image, text_tensor, label)

train_data = CustomImageTextDataset(image_path / "train", transform=data_transform, embedding_dict = embed_dict)
test_data = CustomImageTextDataset(image_path / "test", transform=data_transform, embedding_dict = test_embed_dict)
print(f"Train data:\n{train_data}\nTest data:\n{test_data}")

# Get class names as a list
# class_names = train_data.classes
# class_names
class_names = ['art_nouveau', 'baroque', 'expressionism', 'impressionism', 'post_impressionism', 'realism', 'renaissance', 'romanticism', 'surrealism', 'ukiyo_e']

"""### Creating a subset of the Artbench dataset

We will create a subset of the Artbench dataset, say 20% of the data. This subset will be used to feed feature extractor models.

The full dataset will be used to fine tune the models
"""

def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42,device_=device):
    """Randomly splits a given dataset into two subsets based on split_size and seed.

    Args:
        dataset (torchvision.datasets): A PyTorch Dataset.
        split_size (float): the split size, defaults = 0.2.
        seed (int): Seed for random generator, defaults = 42.

    Returns:
        subset_1, subset_2 (tuple): the two subsets.
    """
    # Create split lengths
    length_1 = int(len(dataset) * split_size)  # length of subset_1
    length_2 = len(dataset) - length_1 # length of subset_2

    # Print out info
    print(f"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)")

    # Create splits with given random seed
    subset_1, subset_2 = torch.utils.data.random_split(dataset,
                                                       lengths=[length_1, length_2],
                                                       generator=torch.manual_seed(42)
                                                      )
    return subset_1, subset_2

# Create training 20% split of Artbench dataset
train_data_20_percent, _ = split_dataset(dataset=train_data, split_size=0.2)

# Create testing 20% split of Artbench dataset
test_data_20_percent, _ = split_dataset(dataset=test_data, split_size=0.2)

"""### Turn loaded images into `DataLoader`

The last step in preparing the data is to convert our Dataset into a DataLoader.

We can use [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to convert our Dataset into a Python iterable, which we will use to iterate over batches. This method also supports automatic shuffling, and memory pinning.
"""

# split part of data as unlabeled data

# full dataset
train_DataLoader = DataLoader(dataset=train_data, batch_size=FLAGS['batch_size'],
                              num_workers=FLAGS['num_workers'], shuffle=True)
test_DataLoader = DataLoader(dataset=test_data, batch_size=FLAGS['batch_size'],
                             num_workers=FLAGS['num_workers'], shuffle=False)
# test_DataLoader = DataLoader(dataset=train_data, batch_size=FLAGS['batch_size'],
#                              num_workers=FLAGS['num_workers'], shuffle=False)

# 20% of the data
train_DataLoader_20_percent = DataLoader(train_data_20_percent,batch_size=FLAGS['batch_size'],
                                         num_workers=FLAGS['num_workers'], shuffle=True)
test_DataLoader_20_percent = DataLoader(test_data_20_percent,batch_size=FLAGS['batch_size'],
                                        num_workers=FLAGS['num_workers'], shuffle=False)

print(f"Num batches using all data:  train: {len(train_DataLoader)} | test: {len(test_DataLoader)}")
print(f"Num batches 20% of the data: train: {len(train_DataLoader_20_percent)}  | test: {len(test_DataLoader_20_percent)}")

for i, (images, texts, labels) in enumerate(train_DataLoader):
  print(texts.size())
  break

"""### Data Preparation - Put it all together

Let's put it all together and create a function for data preparation:
1. Provide the path for the training directory and testing directory.
2. Turn them into PyTorch Datasets and then into PyTorch DataLoaders.
"""

import os

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

def create_dataloaders(
    train_dir: str,
    test_dir: str,
    transform: transforms.Compose,
    batch_size: int,
    num_workers: int=FLAGS['num_workers'],
    split_size: int=1, # if split_size<1, Create a subset of the dataset
    embedding_dict: dict=None,
    test_embedding_dict: dict=None
):
  """Creates training and testing DataLoaders.

  Args:
    train_dir: Path to training directory.
    test_dir: Path to testing directory.
    transform: torchvision transforms to perform on training and testing data.
    batch_size: Number of samples per batch in each of the DataLoaders.
    num_workers: An integer for number of workers per DataLoader.
    split_size (float): the split size, default = 1 (no split)

  Returns:
    A tuple of (train_dataloader, test_dataloader, class_names)
    where class_names is a list of the target classes.
  """
  # 1. Create datasets
  # train_data = datasets.ImageFolder(train_dir, transform=transform)
  # test_data = datasets.ImageFolder(test_dir, transform=transform)
  train_data = CustomImageTextDataset(train_dir, transform=transform, embedding_dict = embedding_dict)
  test_data = CustomImageTextDataset(test_dir, transform=transform, embedding_dict = test_embedding_dict)
  print(f"Data size - Train: {len(train_data)} | Test: {len(test_data)}")

  # # 2. Get class names
  # class_names = train_data.classes

  # 3. Create a subset of the dataset
  if split_size<1:
    train_data, unlabel_data = split_dataset(dataset=train_data, split_size=split_size)
    # test_data, _ =  split_dataset(dataset=test_data, split_size=split_size)
    test_data, _ =  split_dataset(dataset=test_data, split_size=1)
    print(f"Subset size - Train: {len(train_data)} | Test: {len(test_data)}")

  # 4. Create train and test DataLoaders
  train_dataloader = DataLoader(
      train_data,
      batch_size=batch_size,
      shuffle=True,
      # num_workers=num_workers,
      # pin_memory=True,
  )
  test_dataloader = DataLoader(
      test_data,
      batch_size=batch_size,
      shuffle=False,
      # num_workers=num_workers,
      # pin_memory=True,
  )
  print(f"Number of batches of the dataloaders - Train: {len(train_dataloader)} | Test: {len(test_dataloader)}")

  return train_dataloader, test_dataloader

"""### Print a summary using torchinfo"""

def get_model_summary(model,input_size=(1,3,FLAGS['IMAGE_SIZE'],FLAGS['IMAGE_SIZE'])):
  """print and return the model summary using torchinfo.summary()"""
  model_summary = torchinfo.summary(model,
                                    input_size=input_size,
                                    col_names=["input_size","output_size","num_params","trainable"],
                                    col_width=15)
  print(model_summary)
  return model_summary

"""### Create DataLoaders using custom transforms (Resize + TrivialAugmentWide)"""

data_transform = transforms.Compose([
    transforms.Resize(size=(FLAGS['IMAGE_SIZE'], FLAGS['IMAGE_SIZE'])),
    transforms.TrivialAugmentWide(num_magnitude_bins=8), # Data Augmentation
    transforms.ToTensor()
])

train_DataLoader,test_DataLoader = create_dataloaders(train_dir=image_path / "train",
                                                                  test_dir=image_path / "test",
                                                                  transform=data_transform,
                                                                  batch_size=FLAGS['batch_size'],
                                                                  split_size=1,
                                                                  embedding_dict=embed_dict,
                                                                  test_embedding_dict=test_embed_dict)

"""### Create Accuracy function"""

def accuracy_fn(y_true, y_pred):
    """calculate accuracy between truth labels and predictions.

    Args:
        y_true (torch.Tensor): Truth labels.
        y_pred (torch.Tensor): Predictions.

    Returns:
        [torch.float]: Accuracy value between y_true and y_pred.
    """
    correct = torch.eq(y_true, y_pred).sum().item()
    acc = (correct / len(y_pred)) * 100
    return acc

"""### Create train & test step functions

we will create three functions:
1. `train_step()` - trains the model on the train_DataLoader.
2. `test_step()` - evaluates the model on the test_DataLoader.
3. `train_and_evaluate()` - Creates a train and test loop using train_step and test_step. It trains, evaluates the model and tracks the results.

"""



"""CLIP model"""

# from PIL import Image
# import requests

# from transformers import CLIPProcessor, CLIPModel
# from transformers import BlipProcessor, BlipForConditionalGeneration

# processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
# model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'
# raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# # unconditional image captioning
# inputs = processor(raw_image, return_tensors="pt")

# out = model.generate(**inputs)
# print(processor.decode(out[0], skip_special_tokens=True))

class_names

# captions = {}
# for i in class_names:
#   class_dict = {}
#   image_path_list = list(image_path.glob(f"train/{i}/*.jpg"))
#   for image in image_path_list:
#     img = Image.open(image).convert('RGB')
#     inputs = processor(img, return_tensors="pt")
#     out = model.generate(**inputs)
#     class_dict[image.name] = processor.decode(out[0], skip_special_tokens=True)
#     break
#   captions[i] = class_dict
#     # print(processor.decode(out[0], skip_special_tokens=True))

"""#### Create a train step function

This function traines the model for one epoch using all batches in the train_DataLoader. Once the epoch is complete, the train_loss and train_accuracy are returned.
"""

def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               MLP_layer: torch.nn.Linear):
    """Train the model on the dataloader"""

    # Put model in train mode
    model.train()

    # Setup train loss and train accuracy values
    train_loss, train_acc = 0, 0

    # Loop through DataLoader batches
    for batch, (X, text, y) in enumerate(dataloader):
        # 1. Send data to target device
        X, text, y = X.to(device), text.to(device), y.to(device)

        # 2. Forward pass

        Vision_embedding = model(X)

        concat_embed = torch.concat([Vision_embedding, text.squeeze(1)], axis = 1)
        pred_logits = MLP_layer(concat_embed)

        # pred_logits = model(X, text)
        pred_labels = torch.argmax(torch.softmax(pred_logits, dim=1), dim=1)

        # 3. Calculate and accumulate loss
        loss = loss_fn(pred_logits, y)
        train_loss += loss.item()

        # 4. Calculate and accumulate accuracy
        acc = accuracy_fn(y,pred_labels)
        train_acc += acc

        # 5. Gradient Descent
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # 6. print train loss and accuracy per batch
        if batch % FLAGS['show_results_every_X_batch'] == 0:
          print(f"batch: {batch} | train_loss: {loss.item()} | train_acc: {acc}")

    # 7. Get average loss and accuracy per batch
    train_loss = train_loss / len(dataloader)
    train_acc = train_acc / len(dataloader)
    return train_loss, train_acc

"""#### Create a test loop function"""

def test_step(model: torch.nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              MLP_layer: torch.nn.Linear):
    """Evaluate the model on the dataloader"""
    # Put model in eval mode
    model.eval()

    # Setup test loss and test accuracy values
    test_loss, test_acc = 0, 0

    # Turn on inference context manager
    with torch.inference_mode():
        # Loop through DataLoader batches
        for batch, (X, text, y) in enumerate(dataloader):
            # 1. Send data to target device
            X, text, y = X.to(device), text.to(device), y.to(device)
            Vision_embedding = model(X)

            concat_embed = torch.concat([Vision_embedding, text.squeeze(1)], axis = 1)
            test_pred_logits = MLP_layer(concat_embed)

            # pred_logits = model(X, text)
            test_pred_labels = test_pred_logits.argmax(dim=1)
            # # 2. Forward pass
            # test_pred_logits = model(X)
            # test_pred_labels = test_pred_logits.argmax(dim=1)

            # 3. Calculate and accumulate loss
            loss = loss_fn(test_pred_logits, y)
            test_loss += loss.item()

            # 4. Calculate and accumulate accuracy
            test_acc += accuracy_fn(y,test_pred_labels)

    # 5. Get average loss and accuracy per batch
    test_loss = test_loss / len(dataloader)
    test_acc = test_acc / len(dataloader)
    return test_loss, test_acc

"""#### Combine train and test loop functions

Let's combine the train and test loop function in one function: train_and_evaluate.

We will use CrossEntropyLoss as the loss function and Adam as the optimizer. The learning rate for the Adam optimizer is set to 0.001 by default.
"""

from tqdm.auto import tqdm

def train_and_evaluate(model: torch.nn.Module,
                       train_dataloader: torch.utils.data.DataLoader,
                       test_dataloader: torch.utils.data.DataLoader,
                       epochs: int = 5,
                       learning_rate = 0.0001,
                       device = None):
    """Train the model for a number of epochs, evaluate the model and track the results."""

    # Create empty results dictionary
    results = {"train_loss": [],
      "train_acc": [],
      "test_loss": [],
      "test_acc": []
      }

    # Set random seeds
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)

    # Setup loss function and optimizer
    loss_fn = nn.CrossEntropyLoss()
    # optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)
    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate,weight_decay = 1e-6)
    MLP_layer = nn.Linear(in_features=model.hidden_dim * 2,out_features=10).to(device)
    # Start the timer
    from timeit import default_timer as timer
    start_time = timer()

    # Loop through training and testing steps for a number of epochs
    for epoch in tqdm(range(epochs)):
        print(f"\nepoch: {epoch}\n--------------------------")
        train_loss, train_acc = train_step(model=model,
                                            dataloader=train_dataloader,
                                            loss_fn=loss_fn,
                                            optimizer=optimizer,
                                           MLP_layer=MLP_layer)
        test_loss, test_acc = test_step(model=model,
            dataloader=test_dataloader,
            loss_fn=loss_fn,
            MLP_layer=MLP_layer)

        # Print results per epoch
        print(
            f"Epoch: {epoch} | "
            f"train_loss: {train_loss:.4f} | "
            f"train_acc: {train_acc:.4f} | "
            f"test_loss: {test_loss:.4f} | "
            f"test_acc: {test_acc:.4f}"
        )

        # Update results dictionary
        results["train_loss"].append(train_loss)
        results["train_acc"].append(train_acc)
        results["test_loss"].append(test_loss)
        results["test_acc"].append(test_acc)

    end_time = timer()
    print(f"Total training time: {end_time-start_time:.3f} seconds")

    return results

"""### Save the results to CSV"""

def save_results_to_CSV(model_results,model_name,split_size,results_save_path):
  model_results_pd = pd.DataFrame(model_results)
  model_results_pd['model_name'] = model_name
  model_results_pd['data'] = split_size
  print(f"Saving results to: {results_save_path}")
  model_results_pd.to_csv(results_save_path,index=False)

"""### Plot loss curves"""

def plot_loss_curves(results,graph_name="fig",save_graph=False,
                     PATH_results=PATH_results,suptitle=""):
    """Plots training curves of a results dictionary.

    Args:
        results (dict): dictionary containing list of values, e.g. {"train_loss": [...], "train_acc": [...],...}
        graph_name (str): the name of the graph
        save_graph (boolean): if True, the graph will be saved
        PATH_results: the path where the graph will be stored
        suptitle (str): add a suptitle, default None
    """
    loss = results["train_loss"]
    test_loss = results["test_loss"]

    accuracy = results["train_acc"]
    test_accuracy = results["test_acc"]

    epochs = range(len(results["train_loss"]))

    plt.figure(figsize=(15, 7))

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, label="train_loss")
    plt.plot(epochs, test_loss, label="test_loss")
    plt.title("Loss")
    plt.xlabel("Epochs")
    plt.legend()

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(epochs, accuracy, label="train_accuracy")
    plt.plot(epochs, test_accuracy, label="test_accuracy")
    plt.title("Accuracy")
    plt.xlabel("Epochs")
    plt.legend()
    if suptitle!="":
        plt.suptitle(suptitle, fontsize=18)

    # Save the figure
    if save_graph:
        graph_name = graph_name+".jpg"
        plt.savefig(PATH_results / graph_name)

"""The TinyVGG model performed poorly with an accuracy of 31%, but this is better than guessing (if our model were to guess, the accuracy would be 10%).

We could try to train the model for longer and see what happens.

### Save the model

We will save model's `state_dict()` using `torch.save()`.
"""

def save_model(model,MODEL_SAVE_PATH):
  """Save the model state dict: only saves the learned parameters"""

  print(f"Saving model to: {MODEL_SAVE_PATH}")
  torch.save(obj=model.state_dict(),
             f=MODEL_SAVE_PATH)

"""### Make predictions and calculate average prediction time

Let's create a function called `pred_and_timing()` to iterate over test images and find the average prediction time.
"""

# Get all test data paths
test_dir = image_path / "test"
test_data_paths = list(Path(test_dir).glob("*/*.jpg"))

# Get 200 random images
random_image_paths = random.sample(test_data_paths,200)
print(f"[INFO] Random selection of 200 images from {test_dir}")

import pathlib
import torch

from PIL import Image
from timeit import default_timer as timer
from tqdm.auto import tqdm
from typing import List, Dict

def pred_and_timing(paths: List[pathlib.Path],
                   model: torch.nn.Module,
                   transform: torchvision.transforms,
                   class_names: List[str],
                   device: str = "cuda" if torch.cuda.is_available() else "cpu") -> List[Dict]:
    print("\nMake predictions and calculate average prediction time...")

    pred_list = [] # list to store prediction dictionaires

    for path in tqdm(paths):

        pred_dict = {} # dict to store prediction information (path, class_name, pred_prob, pred_class and time_for_pred)

        # Set path and class_name
        pred_dict["image_path"] = path
        class_name = path.parent.stem
        pred_dict["class_name"] = class_name

        start_time = timer() # 5. Start the prediction timer

        # Open image path
        img = Image.open(path)

        # Transform the image and add batch dimension
        transformed_image = transform(img).unsqueeze(0).to(device)

        # Prepare model for inference
        model.to(device)
        model.eval()

        # Get prediction probability, predicition label and prediction class
        with torch.inference_mode():
            pred_logit = model(transformed_image) # prediction logits
            pred_prob = torch.softmax(pred_logit, dim=1) # prediction probabilities
            pred_label = torch.argmax(pred_prob, dim=1) # prediction labels
            pred_class = class_names[pred_label.cpu()]

            pred_dict["pred_prob"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)
            pred_dict["pred_class"] = pred_class

            # Calculate prediction time
            end_time = timer()
            pred_dict["time_for_pred"] = round(end_time-start_time, 4)

        # Was the prediction right?
        pred_dict["correct"] = class_name == pred_class

        # Add the dict to the list of preds
        pred_list.append(pred_dict)

        # Convert to dataframe
        pred_df = pd.DataFrame(pred_list)

        # Calculate average prediction time
        avg_prediction_time = round(pred_df.time_for_pred.mean(), 4)

    return pred_list,pred_df,avg_prediction_time

"""> As we will be deploying our application on HuggingFace Spaces and using free CPUs, we will check the average prediction time of our model using the CPU device.

### Collect model stats

The stats include:
1. model size in Bytes
2. number of parameters
3. test loss and accuracy
4. average prediction time
"""

def get_model_stats(model_name,MODEL_SAVE_PATH,model_summary,model_results,avg_prediction_time):
  """return model size in megabytes, number of parameters, test loss/accuray and avg_prediction_time"""

  model_size = np.round(MODEL_SAVE_PATH.stat().st_size / (1024*1024),1)
  num_parameters = model_summary.total_params
  # Create model statistics dictionary
  model_stats = {"model_name":model_name,
                 "test_loss": np.round(model_results["test_loss"][-1],3),
                 "test_acc": np.round(model_results["test_acc"][-1],3),
                 "number_of_parameters": num_parameters,
                 "model_size (MB)": model_size,
                 "avg_prediction_time":avg_prediction_time}
  return model_stats



"""## <a class="anchor" id="vit_b16_FE">Transformer model - ViT_B16: Feature Extraction</a>

We will create a ViT feature extractor model
 leveraging [`torchvision.models.vit_b_16()`](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16)

1. Freeze all layers of the model
2. Change the classifier head (set out_features to 10 classes)
"""

# Check out ViT heads layer
vit = torchvision.models.vit_b_16()
vit

"""### Build the ViT model"""

def create_vit_model(num_classes:int=10,
                     seed:int=42,
                     is_TrivialAugmentWide = True,
                     freeze_layers=True
                     ):
    """Creates a ViT_B16 feature extractor model and transforms.

    Args:
        num_classes (int): number of target classes, default = 10.
        seed (int, optional): random seed value for output layer, default = 42.
        is_TrivialAugmentWide (boolean): Artificially increase the diversity of a training dataset with data augmentation, default = True.
        freeze_layers (boolean): freeze all layers in model, default = True.

    Returns:
        vit_b16_model (torch.nn.Module): ViT_B16 feature extractor model.
        vit_b16_transforms (torchvision.transforms): ViT_B16 image transforms.
    """
    # Create ViT_B_16 pretrained weights and transforms
    weights = torchvision.models.ViT_B_16_Weights.DEFAULT
    vit_b16_transforms = weights.transforms()
    # add data augmentation
    if is_TrivialAugmentWide:
      vit_b16_transforms = torchvision.transforms.Compose([
          torchvision.transforms.TrivialAugmentWide(num_magnitude_bins=8),
          vit_b16_transforms,
      ])
    # create the VIT_B16 model
    vit_b16_model = torchvision.models.vit_b_16(weights=weights)

    # Freeze all layers in the model
    if freeze_layers:
        for param in vit_b16_model.parameters():
            param.requires_grad = False

    # Change classifier head to reflect target number of classes
    # torch.manual_seed(seed)
    vit_b16_model.heads = nn.Sequential(nn.Linear(in_features=768,
                                                  out_features=768))

    return vit_b16_model, vit_b16_transforms

# Create ViT feature extractor model and transforms
model_ViT, vit_transforms = create_vit_model(num_classes=10,
                                             seed=42,
                                             is_TrivialAugmentWide=True)

# Print ViT feature extractor model summary
summary_ViT = get_model_summary(model_ViT)

"""### Create dataloaders using ViT transforms"""

# Train and test the ViT feature extractor on 20% of the data
train_DataLoader,test_DataLoader = create_dataloaders(train_dir=image_path / "train",
                                                                  test_dir=image_path / "test",
                                                                  transform=vit_transforms,
                                                                  batch_size=FLAGS['batch_size'],
                                                                  split_size=0.2,
                                                                  embedding_dict = embed_dict,
                                                                  test_embedding_dict=test_embed_dict)

"""### Train, evaluate and save results of the ViT_B16 feature extractor"""

model_ViT =model_ViT.to(device)

# 1. train and evaluate the model
results_ViT_20percent = train_and_evaluate(model=model_ViT,
                                           train_dataloader=train_DataLoader,
                                           test_dataloader=test_DataLoader,
                                           epochs=5,
                                           learning_rate = 0.0001,
                                           device=device)

# 2. Plot loss curves
plot_loss_curves(results_ViT_20percent,
                 graph_name="fig_ViT_20percent",save_graph=True)

# 3. Save results
save_results_to_CSV(model_results=results_ViT_20percent,
                    model_name = "ViT_B16",
                    split_size = 0.2,
                    results_save_path = PATH_results / "results_ViT_B16_20percent.csv")

# 4. Save the model
save_model(model = model_ViT,
           MODEL_SAVE_PATH = PATH_models / "ViT_B16_20percent.pth"
           )

# # 5. Make predictions and calculate avg prediction time
# ViT_test_pred_dicts,ViT_test_pred_df,ViT_avg_prediction_time =\
# pred_and_timing(paths=random_image_paths,
#                 model=model_ViT,
#                 transform=vit_transforms,
#                 class_names=class_names,
#                 device="cpu") # make predictions on CPU

# print("ViT_avg_prediction_time:",ViT_avg_prediction_time)

# # 6. Collect model stats
# stats_ViT_20percent = get_model_stats(model_name = "ViT_B16",
#                                       MODEL_SAVE_PATH = PATH_models / "ViT_B16_20percent.pth",
#                                       model_summary= summary_ViT,
#                                       model_results = results_ViT_20percent,
#                                       avg_prediction_time= ViT_avg_prediction_time
#                                       )
# # 7. Save the stats to csv
# pd.DataFrame([stats_ViT_20percent]).to_csv(PATH_results / "stats_ViT_20percent.csv",index=False)
# print("\nstats_ViT_20percent:")
# stats_ViT_20percent

"""The ViT_B16 feature extractor achieved over **49%** accuracy on test dataset.

## <a class="anchor" id="vit_b16_FT">IV. ViT_B16: Fine Tuning</a>

1. Create a new instance of ViT_B16 with all its layers trainable.
2. Load the state_dict of our saved feature extractor model. This will update the new instance of our model with trained weights
3. Train the model for an additional 5 epochs on 50% of the dataset (to make the training time faster).
"""

# 1. Create a new instance of ViT model and ViT transforms
model_ViT, vit_transforms = create_vit_model(num_classes=len(class_names),
                                             seed=42,
                                             is_TrivialAugmentWide=True,
                                             freeze_layers=False)
summary_ViT = get_model_summary(model_ViT)

# 2. Load the state_dict of our saved ViT feature extractor
MODEL_SAVE_PATH = PATH_models / "ViT_B16_20percent.pth"
model_ViT.load_state_dict(torch.load(f=MODEL_SAVE_PATH))

# 3. Create dataloaders using ViT_B16 transforms and 100% of the data
train_DataLoader,test_DataLoader = create_dataloaders(train_dir=image_path / "train",
                                                                  test_dir=image_path / "test",
                                                                  transform=vit_transforms,
                                                                  batch_size=FLAGS['batch_size'],
                                                                  split_size=1,
                                                                  embedding_dict=embed_dict,
                                                                  test_embedding_dict=test_embed_dict)

# 4. train and evaluate the model
torch.manual_seed(42)
torch.cuda.manual_seed(42)
model_ViT = model_ViT.to(device)
results_ViT_FT = train_and_evaluate(model=model_ViT,
                                    train_dataloader=train_DataLoader,
                                    test_dataloader=test_DataLoader,
                                    epochs=20,
                                    learning_rate=0.00001,
                                    device=device)

# 5. Save results
save_results_to_CSV(model_results=results_ViT_FT,
                    model_name = "ViT_B16",
                    split_size = 1,
                    results_save_path = PATH_results / "results_ViT_B16_FT.csv")

# 6. Save the model
save_model(model = model_ViT,
           MODEL_SAVE_PATH = PATH_models / "ViT_B16_FT.pth"
           )

# # 7. Make predictions and calculate avg prediction time
# ViT_test_pred_dicts,ViT_test_pred_df,ViT_avg_prediction_time =\
# pred_and_timing(paths=random_image_paths,
#                 model=model_ViT,
#                 transform=vit_transforms,
#                 class_names=class_names,
#                 device="cpu") # make predictions on CPU

# print("ViT_avg_prediction_time:",ViT_avg_prediction_time)

# # 8. Collect model stats
# stats_ViT_FT = get_model_stats(model_name = "ViT_B16",
#                                MODEL_SAVE_PATH = PATH_models / "ViT_B16_FT.pth",
#                                model_summary= summary_ViT,
#                                model_results = results_ViT_FT,
#                                avg_prediction_time= ViT_avg_prediction_time
#                               )
# # 9. Save the stats to csv
# pd.DataFrame([stats_ViT_FT]).to_csv(PATH_results / "stats_ViT_FT.csv",index=False)
# print("\nstats_ViT_FT:")
# stats_ViT_FT

def compare_results(FeatureExtraction_results, FineTuning_results,
                    initial_epochs=5,fig_name="fig",save_fig=False,
                    PATH_results=PATH_results,suptitle=""):
    """
    Returns two plots: Accuracy and Loss. Compares the results of the feature extractor and after fine tuning.

    Args:
      FeatureExtraction_results: results of the feature extractor.
      FineTuning_results: results after fine tuning.
      initial_epochs: Number of epochs of the feature extractor.
      fig_name: Name of the figure
      save_fig (boolean): if True, save the figure.
      suptitle (str): suptitle
    """

    # Get Feature Extraction results
    acc = FeatureExtraction_results["train_acc"]
    loss = FeatureExtraction_results["train_loss"]

    val_acc = FeatureExtraction_results["test_acc"]
    val_loss = FeatureExtraction_results["test_loss"]

    # Combine Feature Extraction results with Fine tuning results
    total_acc = acc + FineTuning_results["train_acc"]
    total_loss = loss + FineTuning_results["train_loss"]

    total_val_acc = val_acc + FineTuning_results["test_acc"]
    total_val_loss = val_loss + FineTuning_results["test_loss"]

    # Plot loss curves
    plt.figure(figsize=(14, 5))
    plt.subplot(1, 2, 1)
    plt.plot(total_acc, label='train_accuracy')
    plt.plot(total_val_acc, label='test_accuracy')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='lower right')
    plt.ylabel("%")
    plt.title('Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(total_loss, label='train_loss')
    plt.plot(total_val_loss, label='test_loss')
    plt.plot([initial_epochs-1, initial_epochs-1],
              plt.ylim(), label='Start Fine Tuning') # reshift plot around epochs
    plt.legend(loc='upper right')
    plt.title('Loss')
    plt.xlabel('epoch')

    if suptitle!="":
        plt.suptitle(suptitle, fontsize=18)


    if save_fig:
        fig_name = fig_name+".jpg"
        plt.savefig(PATH_results / fig_name)

    plt.show()

# 10. Plot loss curves - Compare Feature extraction vs. Fine tuning

results_ViT_20percent = pd.read_csv(PATH_results / "results_ViT_B16_20percent.csv").to_dict(orient='list')
results_ViT_FT = pd.read_csv(PATH_results / "results_ViT_B16_FT.csv").to_dict(orient='list')

compare_results(results_ViT_20percent,results_ViT_FT,
                initial_epochs=5,
                fig_name="fig_ViT_B16_FT",save_fig=True,
                suptitle="ViT_B16 Finetuning")

